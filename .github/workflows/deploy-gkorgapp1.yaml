# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Opsera Unified AWS Container Deployment - GKOrgApp1
# Generated: 2026-01-10 | Version: 1.13.0
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Tenant: GKOrg | App: gkorgapp1 | Region: us-west-2 | Environment: dev
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

name: Deploy GKOrgApp1 to AWS EKS

on:
  workflow_dispatch:
    inputs:
      tenant_name:
        description: 'Tenant/Organization name'
        required: true
        type: string
        default: 'gkorg'
      app_name:
        description: 'Application name'
        required: true
        type: string
        default: 'gkorgapp1'
      app_env:
        description: 'Target environment'
        required: true
        type: choice
        options: [dev, staging, prod]
        default: 'dev'
      app_region:
        description: 'AWS deployment region'
        required: true
        type: string
        default: 'us-west-2'

permissions:
  contents: write
  id-token: write
  actions: read

# Short naming convention (Learning #118)
# Region: us-west-2 -> usw2 | Cluster env: dev -> np (nonprod)
env:
  DEPLOY_BRANCH: ${{ inputs.app_name }}-deploy
  REGION_SHORT: usw2
  CLUSTER_ENV_SHORT: ${{ inputs.app_env == 'prod' && 'prod' || 'np' }}
  VPC_NAME: opsera-vpc
  ARGOCD_CLUSTER: argocd-usw2
  WORKLOAD_CLUSTER: ${{ inputs.tenant_name }}-usw2-${{ inputs.app_env == 'prod' && 'prod' || 'np' }}
  ECR_REPO: ${{ inputs.tenant_name }}/${{ inputs.app_name }}
  NAMESPACE: ${{ inputs.app_name }}-${{ inputs.app_env }}
  APP_PORT: "5000"

jobs:
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # PHASE 1: INFRASTRUCTURE
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  infrastructure:
    name: "Phase 1: Infrastructure"
    runs-on: ubuntu-latest
    outputs:
      argocd_cluster: ${{ steps.discover.outputs.argocd_cluster }}
      workload_cluster: ${{ steps.discover.outputs.workload_cluster }}
      ecr_repo: ${{ steps.discover.outputs.ecr_repo }}
      namespace: ${{ steps.discover.outputs.namespace }}
      deployment_type: ${{ steps.discover.outputs.deployment_type }}
      aws_account_id: ${{ steps.discover.outputs.aws_account_id }}
      ecr_registry: ${{ steps.discover.outputs.ecr_registry }}

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          ref: ${{ github.ref }}
          fetch-depth: 0

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ inputs.app_region }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.6.0"

      - name: Discover Infrastructure
        id: discover
        run: |
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "  INFRASTRUCTURE DISCOVERY - GKOrgApp1"
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query 'Account' --output text)
          ECR_REGISTRY="${AWS_ACCOUNT_ID}.dkr.ecr.${{ inputs.app_region }}.amazonaws.com"
          
          echo "aws_account_id=$AWS_ACCOUNT_ID" >> $GITHUB_OUTPUT
          echo "ecr_registry=$ECR_REGISTRY" >> $GITHUB_OUTPUT

          ARGOCD_CLUSTER="${{ env.ARGOCD_CLUSTER }}"
          WORKLOAD_CLUSTER="${{ env.WORKLOAD_CLUSTER }}"
          ECR_REPO="${{ env.ECR_REPO }}"
          NAMESPACE="${{ env.NAMESPACE }}"

          # Force lowercase for ECR (Learning #120)
          ECR_REPO=$(echo "$ECR_REPO" | tr '[:upper:]' '[:lower:]')

          echo "argocd_cluster=$ARGOCD_CLUSTER" >> $GITHUB_OUTPUT
          echo "workload_cluster=$WORKLOAD_CLUSTER" >> $GITHUB_OUTPUT
          echo "ecr_repo=$ECR_REPO" >> $GITHUB_OUTPUT
          echo "namespace=$NAMESPACE" >> $GITHUB_OUTPUT

          echo ""
          echo "Checking VPC: ${{ env.VPC_NAME }}..."
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=${{ env.VPC_NAME }}" \
            --query 'Vpcs[0].VpcId' --output text 2>/dev/null || echo "None")
          if [ "$VPC_ID" != "None" ] && [ -n "$VPC_ID" ] && [ "$VPC_ID" != "null" ]; then
            VPC_EXISTS="true"
            echo "  âœ… VPC EXISTS: $VPC_ID"
          else
            VPC_EXISTS="false"
            echo "  âš ï¸  VPC NOT FOUND - will create"
          fi
          echo "vpc_exists=$VPC_EXISTS" >> $GITHUB_OUTPUT
          echo "vpc_id=$VPC_ID" >> $GITHUB_OUTPUT

          echo ""
          echo "Checking ArgoCD Cluster: $ARGOCD_CLUSTER..."
          if aws eks describe-cluster --name "$ARGOCD_CLUSTER" --region ${{ inputs.app_region }} 2>/dev/null; then
            ARGOCD_EXISTS="true"
            echo "  âœ… ArgoCD cluster EXISTS"
          else
            ARGOCD_EXISTS="false"
            echo "  âš ï¸  ArgoCD cluster NOT FOUND - will create"
          fi
          echo "argocd_exists=$ARGOCD_EXISTS" >> $GITHUB_OUTPUT

          echo ""
          echo "Checking Workload Cluster: $WORKLOAD_CLUSTER..."
          if aws eks describe-cluster --name "$WORKLOAD_CLUSTER" --region ${{ inputs.app_region }} 2>/dev/null; then
            WORKLOAD_EXISTS="true"
            echo "  âœ… Workload cluster EXISTS"
          else
            WORKLOAD_EXISTS="false"
            echo "  âš ï¸  Workload cluster NOT FOUND - will create"
          fi
          echo "workload_exists=$WORKLOAD_EXISTS" >> $GITHUB_OUTPUT

          echo ""
          echo "Checking ECR Repository: $ECR_REPO..."
          if aws ecr describe-repositories --repository-names "$ECR_REPO" --region ${{ inputs.app_region }} 2>/dev/null; then
            ECR_EXISTS="true"
            echo "  âœ… ECR repository EXISTS"
          else
            ECR_EXISTS="false"
            echo "  âš ï¸  ECR repository NOT FOUND - will create"
          fi
          echo "ecr_exists=$ECR_EXISTS" >> $GITHUB_OUTPUT

          # Determine deployment type
          if [ "$ARGOCD_EXISTS" = "true" ] && [ "$WORKLOAD_EXISTS" = "true" ]; then
            DEPLOY_TYPE="brownfield"
          elif [ "$ARGOCD_EXISTS" = "true" ]; then
            DEPLOY_TYPE="partial"
          else
            DEPLOY_TYPE="greenfield"
          fi
          echo "deployment_type=$DEPLOY_TYPE" >> $GITHUB_OUTPUT

          echo ""
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "  DEPLOYMENT TYPE: $DEPLOY_TYPE"
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

      # Learning #114: Region-specific Terraform state bucket
      - name: Setup Terraform State Backend
        id: tf_backend
        run: |
          echo "Setting up Terraform State Backend..."

          AWS_ACCOUNT_ID=${{ steps.discover.outputs.aws_account_id }}
          REGION="${{ inputs.app_region }}"

          # Region-specific bucket name (Learning #114)
          BUCKET_NAME="opsera-tf-state-${AWS_ACCOUNT_ID}-${REGION}"
          TABLE_NAME="terraform-state-lock-${REGION}"

          echo "tf_bucket=$BUCKET_NAME" >> $GITHUB_OUTPUT
          echo "tf_table=$TABLE_NAME" >> $GITHUB_OUTPUT

          # Create S3 bucket if not exists
          if ! aws s3api head-bucket --bucket "$BUCKET_NAME" 2>/dev/null; then
            echo "Creating S3 bucket: $BUCKET_NAME in $REGION"
            if [ "$REGION" = "us-east-1" ]; then
              aws s3api create-bucket --bucket "$BUCKET_NAME" --region $REGION
            else
              aws s3api create-bucket \
                --bucket "$BUCKET_NAME" \
                --region $REGION \
                --create-bucket-configuration LocationConstraint=$REGION
            fi

            aws s3api put-bucket-versioning \
              --bucket "$BUCKET_NAME" \
              --versioning-configuration Status=Enabled

            aws s3api put-bucket-encryption \
              --bucket "$BUCKET_NAME" \
              --server-side-encryption-configuration \
              '{"Rules":[{"ApplyServerSideEncryptionByDefault":{"SSEAlgorithm":"AES256"}}]}'
            echo "  âœ… S3 bucket created: $BUCKET_NAME"
          else
            echo "  âœ… S3 bucket exists: $BUCKET_NAME"
          fi

          # Create DynamoDB table if not exists
          if ! aws dynamodb describe-table --table-name "$TABLE_NAME" --region $REGION 2>/dev/null; then
            echo "Creating DynamoDB table: $TABLE_NAME"
            aws dynamodb create-table \
              --table-name "$TABLE_NAME" \
              --attribute-definitions AttributeName=LockID,AttributeType=S \
              --key-schema AttributeName=LockID,KeyType=HASH \
              --billing-mode PAY_PER_REQUEST \
              --region $REGION

            aws dynamodb wait table-exists --table-name "$TABLE_NAME" --region $REGION
            echo "  âœ… DynamoDB table created: $TABLE_NAME"
          else
            echo "  âœ… DynamoDB table exists: $TABLE_NAME"
          fi

      - name: Create ECR Repository
        if: steps.discover.outputs.ecr_exists == 'false'
        run: |
          ECR_REPO="${{ steps.discover.outputs.ecr_repo }}"
          echo "Creating ECR repository: $ECR_REPO"

          aws ecr create-repository \
            --repository-name "$ECR_REPO" \
            --image-scanning-configuration scanOnPush=true \
            --region ${{ inputs.app_region }} || true

          echo "  âœ… ECR repository created: $ECR_REPO"

      # Learning #112: ACTUAL VPC creation with Terraform
      - name: Create VPC with Terraform
        if: steps.discover.outputs.vpc_exists == 'false'
        run: |
          echo "Creating VPC: ${{ env.VPC_NAME }}"

          mkdir -p /tmp/terraform-vpc
          cd /tmp/terraform-vpc

          cat > main.tf << 'EOF'
          terraform {
            required_version = ">= 1.0.0"
            required_providers {
              aws = { source = "hashicorp/aws", version = "~> 5.0" }
            }
            backend "s3" {}
          }

          provider "aws" { region = var.region }

          variable "vpc_name" { type = string }
          variable "region" { type = string }
          variable "vpc_cidr" { type = string, default = "10.0.0.0/16" }
          variable "argocd_cluster" { type = string }
          variable "workload_cluster" { type = string }

          data "aws_availability_zones" "available" { state = "available" }

          resource "aws_vpc" "main" {
            cidr_block           = var.vpc_cidr
            enable_dns_hostnames = true
            enable_dns_support   = true
            tags = { Name = var.vpc_name }
          }

          resource "aws_internet_gateway" "main" {
            vpc_id = aws_vpc.main.id
            tags = { Name = "${var.vpc_name}-igw" }
          }

          resource "aws_subnet" "public" {
            count                   = 2
            vpc_id                  = aws_vpc.main.id
            cidr_block              = cidrsubnet(var.vpc_cidr, 8, count.index)
            availability_zone       = data.aws_availability_zones.available.names[count.index]
            map_public_ip_on_launch = true
            tags = {
              Name = "${var.vpc_name}-public-${count.index + 1}"
              "kubernetes.io/role/elb" = "1"
              "kubernetes.io/cluster/${var.argocd_cluster}" = "shared"
              "kubernetes.io/cluster/${var.workload_cluster}" = "shared"
            }
          }

          resource "aws_subnet" "private" {
            count             = 2
            vpc_id            = aws_vpc.main.id
            cidr_block        = cidrsubnet(var.vpc_cidr, 8, count.index + 10)
            availability_zone = data.aws_availability_zones.available.names[count.index]
            tags = {
              Name = "${var.vpc_name}-private-${count.index + 1}"
              "kubernetes.io/role/internal-elb" = "1"
              "kubernetes.io/cluster/${var.argocd_cluster}" = "shared"
              "kubernetes.io/cluster/${var.workload_cluster}" = "shared"
            }
          }

          resource "aws_eip" "nat" {
            count  = 2
            domain = "vpc"
            tags = { Name = "${var.vpc_name}-nat-${count.index + 1}" }
          }

          resource "aws_nat_gateway" "main" {
            count         = 2
            allocation_id = aws_eip.nat[count.index].id
            subnet_id     = aws_subnet.public[count.index].id
            tags = { Name = "${var.vpc_name}-nat-${count.index + 1}" }
            depends_on = [aws_internet_gateway.main]
          }

          resource "aws_route_table" "public" {
            vpc_id = aws_vpc.main.id
            route { cidr_block = "0.0.0.0/0", gateway_id = aws_internet_gateway.main.id }
            tags = { Name = "${var.vpc_name}-public-rt" }
          }

          resource "aws_route_table" "private" {
            count  = 2
            vpc_id = aws_vpc.main.id
            route { cidr_block = "0.0.0.0/0", nat_gateway_id = aws_nat_gateway.main[count.index].id }
            tags = { Name = "${var.vpc_name}-private-rt-${count.index + 1}" }
          }

          resource "aws_route_table_association" "public" {
            count          = 2
            subnet_id      = aws_subnet.public[count.index].id
            route_table_id = aws_route_table.public.id
          }

          resource "aws_route_table_association" "private" {
            count          = 2
            subnet_id      = aws_subnet.private[count.index].id
            route_table_id = aws_route_table.private[count.index].id
          }

          output "vpc_id" { value = aws_vpc.main.id }
          output "public_subnet_ids" { value = aws_subnet.public[*].id }
          output "private_subnet_ids" { value = aws_subnet.private[*].id }
          EOF

          BUCKET_NAME="${{ steps.tf_backend.outputs.tf_bucket }}"
          TABLE_NAME="${{ steps.tf_backend.outputs.tf_table }}"

          terraform init \
            -backend-config="bucket=${BUCKET_NAME}" \
            -backend-config="key=${{ inputs.app_name }}/vpc/terraform.tfstate" \
            -backend-config="region=${{ inputs.app_region }}" \
            -backend-config="dynamodb_table=${TABLE_NAME}" \
            -backend-config="encrypt=true"

          terraform apply -auto-approve \
            -var="vpc_name=${{ env.VPC_NAME }}" \
            -var="region=${{ inputs.app_region }}" \
            -var="argocd_cluster=${{ env.ARGOCD_CLUSTER }}" \
            -var="workload_cluster=${{ env.WORKLOAD_CLUSTER }}"

          echo "  âœ… VPC created successfully"

      # Create ArgoCD EKS Cluster
      - name: Create ArgoCD EKS Cluster
        if: steps.discover.outputs.argocd_exists == 'false'
        run: |
          echo "Creating ArgoCD EKS Cluster: ${{ env.ARGOCD_CLUSTER }}"

          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=${{ env.VPC_NAME }}" \
            --query 'Vpcs[0].VpcId' --output text)
          SUBNET_IDS=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" \
            "Name=tag:Name,Values=*private*" --query 'Subnets[*].SubnetId' --output text | tr '\t' ',')

          CLUSTER_ROLE_NAME="${{ env.ARGOCD_CLUSTER }}-cluster-role"
          if ! aws iam get-role --role-name "$CLUSTER_ROLE_NAME" 2>/dev/null; then
            aws iam create-role --role-name "$CLUSTER_ROLE_NAME" \
              --assume-role-policy-document '{
                "Version": "2012-10-17",
                "Statement": [{
                  "Effect": "Allow",
                  "Principal": {"Service": "eks.amazonaws.com"},
                  "Action": "sts:AssumeRole"
                }]
              }'
            aws iam attach-role-policy --role-name "$CLUSTER_ROLE_NAME" \
              --policy-arn arn:aws:iam::aws:policy/AmazonEKSClusterPolicy
            sleep 10
          fi

          CLUSTER_ROLE_ARN=$(aws iam get-role --role-name "$CLUSTER_ROLE_NAME" --query 'Role.Arn' --output text)

          aws eks create-cluster \
            --name "${{ env.ARGOCD_CLUSTER }}" \
            --role-arn "$CLUSTER_ROLE_ARN" \
            --resources-vpc-config "subnetIds=$SUBNET_IDS,endpointPublicAccess=true,endpointPrivateAccess=true" \
            --kubernetes-version "1.28" \
            --region ${{ inputs.app_region }} || echo "Cluster creation initiated"

          echo "Waiting for cluster to be ACTIVE (10-15 minutes)..."
          aws eks wait cluster-active --name "${{ env.ARGOCD_CLUSTER }}" --region ${{ inputs.app_region }}
          echo "  âœ… ArgoCD cluster created"

      - name: Create ArgoCD Node Group
        if: steps.discover.outputs.argocd_exists == 'false'
        run: |
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=${{ env.VPC_NAME }}" \
            --query 'Vpcs[0].VpcId' --output text)
          SUBNET_IDS=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" \
            "Name=tag:Name,Values=*private*" --query 'Subnets[*].SubnetId' --output text | tr '\t' ' ')

          NODE_ROLE_NAME="${{ env.ARGOCD_CLUSTER }}-node-role"
          if ! aws iam get-role --role-name "$NODE_ROLE_NAME" 2>/dev/null; then
            aws iam create-role --role-name "$NODE_ROLE_NAME" \
              --assume-role-policy-document '{
                "Version": "2012-10-17",
                "Statement": [{
                  "Effect": "Allow",
                  "Principal": {"Service": "ec2.amazonaws.com"},
                  "Action": "sts:AssumeRole"
                }]
              }'
            aws iam attach-role-policy --role-name "$NODE_ROLE_NAME" \
              --policy-arn arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
            aws iam attach-role-policy --role-name "$NODE_ROLE_NAME" \
              --policy-arn arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
            aws iam attach-role-policy --role-name "$NODE_ROLE_NAME" \
              --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
            sleep 10
          fi

          NODE_ROLE_ARN=$(aws iam get-role --role-name "$NODE_ROLE_NAME" --query 'Role.Arn' --output text)

          # Learning #126: Always specify AMI type
          aws eks create-nodegroup \
            --cluster-name "${{ env.ARGOCD_CLUSTER }}" \
            --nodegroup-name "argocd-nodes" \
            --node-role "$NODE_ROLE_ARN" \
            --subnets $SUBNET_IDS \
            --instance-types t3.medium \
            --ami-type AL2_x86_64 \
            --scaling-config minSize=1,maxSize=3,desiredSize=2 \
            --region ${{ inputs.app_region }} || echo "Node group creation initiated"

          aws eks wait nodegroup-active \
            --cluster-name "${{ env.ARGOCD_CLUSTER }}" \
            --nodegroup-name "argocd-nodes" \
            --region ${{ inputs.app_region }} || true
          echo "  âœ… ArgoCD node group created"

      # Create Workload EKS Cluster
      - name: Create Workload EKS Cluster
        if: steps.discover.outputs.workload_exists == 'false'
        run: |
          echo "Creating Workload EKS Cluster: ${{ env.WORKLOAD_CLUSTER }}"

          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=${{ env.VPC_NAME }}" \
            --query 'Vpcs[0].VpcId' --output text)
          SUBNET_IDS=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" \
            "Name=tag:Name,Values=*private*" --query 'Subnets[*].SubnetId' --output text | tr '\t' ',')

          CLUSTER_ROLE_NAME="${{ env.WORKLOAD_CLUSTER }}-cluster-role"
          if ! aws iam get-role --role-name "$CLUSTER_ROLE_NAME" 2>/dev/null; then
            aws iam create-role --role-name "$CLUSTER_ROLE_NAME" \
              --assume-role-policy-document '{
                "Version": "2012-10-17",
                "Statement": [{
                  "Effect": "Allow",
                  "Principal": {"Service": "eks.amazonaws.com"},
                  "Action": "sts:AssumeRole"
                }]
              }'
            aws iam attach-role-policy --role-name "$CLUSTER_ROLE_NAME" \
              --policy-arn arn:aws:iam::aws:policy/AmazonEKSClusterPolicy
            sleep 10
          fi

          CLUSTER_ROLE_ARN=$(aws iam get-role --role-name "$CLUSTER_ROLE_NAME" --query 'Role.Arn' --output text)

          aws eks create-cluster \
            --name "${{ env.WORKLOAD_CLUSTER }}" \
            --role-arn "$CLUSTER_ROLE_ARN" \
            --resources-vpc-config "subnetIds=$SUBNET_IDS,endpointPublicAccess=true,endpointPrivateAccess=true" \
            --kubernetes-version "1.28" \
            --region ${{ inputs.app_region }} || echo "Cluster creation initiated"

          echo "Waiting for cluster to be ACTIVE (10-15 minutes)..."
          aws eks wait cluster-active --name "${{ env.WORKLOAD_CLUSTER }}" --region ${{ inputs.app_region }}
          echo "  âœ… Workload cluster created"

      - name: Create Workload Node Group
        if: steps.discover.outputs.workload_exists == 'false'
        run: |
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=${{ env.VPC_NAME }}" \
            --query 'Vpcs[0].VpcId' --output text)
          SUBNET_IDS=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" \
            "Name=tag:Name,Values=*private*" --query 'Subnets[*].SubnetId' --output text | tr '\t' ' ')

          NODE_ROLE_NAME="${{ env.WORKLOAD_CLUSTER }}-node-role"
          if ! aws iam get-role --role-name "$NODE_ROLE_NAME" 2>/dev/null; then
            aws iam create-role --role-name "$NODE_ROLE_NAME" \
              --assume-role-policy-document '{
                "Version": "2012-10-17",
                "Statement": [{
                  "Effect": "Allow",
                  "Principal": {"Service": "ec2.amazonaws.com"},
                  "Action": "sts:AssumeRole"
                }]
              }'
            aws iam attach-role-policy --role-name "$NODE_ROLE_NAME" \
              --policy-arn arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
            aws iam attach-role-policy --role-name "$NODE_ROLE_NAME" \
              --policy-arn arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
            aws iam attach-role-policy --role-name "$NODE_ROLE_NAME" \
              --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly

            # Learning #100: ELB permissions for LoadBalancer services
            aws iam put-role-policy --role-name "$NODE_ROLE_NAME" \
              --policy-name "elb-permissions" \
              --policy-document '{
                "Version": "2012-10-17",
                "Statement": [{
                  "Effect": "Allow",
                  "Action": ["elasticloadbalancing:*", "ec2:Describe*", "ec2:CreateSecurityGroup",
                    "ec2:AuthorizeSecurityGroupIngress", "ec2:CreateTags"],
                  "Resource": "*"
                }]
              }'
            sleep 10
          fi

          NODE_ROLE_ARN=$(aws iam get-role --role-name "$NODE_ROLE_NAME" --query 'Role.Arn' --output text)

          # Learning #126: Always specify AMI type
          aws eks create-nodegroup \
            --cluster-name "${{ env.WORKLOAD_CLUSTER }}" \
            --nodegroup-name "workload-nodes" \
            --node-role "$NODE_ROLE_ARN" \
            --subnets $SUBNET_IDS \
            --instance-types t3.medium \
            --ami-type AL2_x86_64 \
            --scaling-config minSize=1,maxSize=4,desiredSize=2 \
            --region ${{ inputs.app_region }} || echo "Node group creation initiated"

          aws eks wait nodegroup-active \
            --cluster-name "${{ env.WORKLOAD_CLUSTER }}" \
            --nodegroup-name "workload-nodes" \
            --region ${{ inputs.app_region }} || true
          echo "  âœ… Workload node group created"

      - name: Install ArgoCD
        if: steps.discover.outputs.argocd_exists == 'false'
        run: |
          curl -LO "https://dl.k8s.io/release/v1.28.0/bin/linux/amd64/kubectl"
          chmod +x kubectl && sudo mv kubectl /usr/local/bin/

          aws eks update-kubeconfig --name "${{ env.ARGOCD_CLUSTER }}" --region ${{ inputs.app_region }}

          kubectl create namespace argocd || true
          kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

          kubectl wait --for=condition=available --timeout=300s deployment/argocd-server -n argocd || true
          echo "  âœ… ArgoCD installed"

      - name: Create OIDC Provider
        if: steps.discover.outputs.workload_exists == 'false'
        run: |
          # Learning #124: OIDC provider required for IRSA
          OIDC_URL=$(aws eks describe-cluster --name "${{ env.WORKLOAD_CLUSTER }}" \
            --query 'cluster.identity.oidc.issuer' --output text)
          OIDC_ID=$(echo $OIDC_URL | cut -d '/' -f5)
          
          if ! aws iam list-open-id-connect-providers | grep -q "$OIDC_ID"; then
            eksctl utils associate-iam-oidc-provider \
              --cluster "${{ env.WORKLOAD_CLUSTER }}" \
              --region ${{ inputs.app_region }} \
              --approve || echo "OIDC provider may already exist"
          fi
          echo "  âœ… OIDC provider configured"

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # PHASE 2: APPLICATION BUILD & PUSH
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  application:
    name: "Phase 2: Application"
    runs-on: ubuntu-latest
    needs: [infrastructure]

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          ref: ${{ github.ref }}
          fetch-depth: 0

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ inputs.app_region }}

      - name: Login to ECR
        run: |
          ECR_REGISTRY="${{ needs.infrastructure.outputs.ecr_registry }}"
          aws ecr get-login-password --region ${{ inputs.app_region }} | \
            docker login --username AWS --password-stdin $ECR_REGISTRY

      - name: Build and Push Docker Image
        run: |
          ECR_REGISTRY="${{ needs.infrastructure.outputs.ecr_registry }}"
          ECR_REPO="${{ needs.infrastructure.outputs.ecr_repo }}"
          IMAGE_TAG="${{ github.sha }}"

          echo "Building image: $ECR_REGISTRY/$ECR_REPO:$IMAGE_TAG"

          # Learning #119: Single container app detection - Dockerfile at root
          if [ -f "Dockerfile" ]; then
            docker build -t $ECR_REGISTRY/$ECR_REPO:$IMAGE_TAG .
            docker push $ECR_REGISTRY/$ECR_REPO:$IMAGE_TAG

            # Learning #134: Tag with BOTH SHA and latest
            docker tag $ECR_REGISTRY/$ECR_REPO:$IMAGE_TAG $ECR_REGISTRY/$ECR_REPO:latest
            docker push $ECR_REGISTRY/$ECR_REPO:latest
            echo "  âœ… Image pushed: $ECR_REGISTRY/$ECR_REPO:$IMAGE_TAG"
            echo "  âœ… Image pushed: $ECR_REGISTRY/$ECR_REPO:latest"
          fi

      - name: Update Kustomization Image Tag
        run: |
          ECR_REGISTRY="${{ needs.infrastructure.outputs.ecr_registry }}"
          ECR_REPO="${{ needs.infrastructure.outputs.ecr_repo }}"
          IMAGE_TAG="${{ github.sha }}"

          # Update kustomization with new image tag
          if [ -f "k8s/overlays/${{ inputs.app_env }}/kustomization.yaml" ]; then
            cd k8s/overlays/${{ inputs.app_env }}
            
            # Update image tag in kustomization
            sed -i "s|newTag:.*|newTag: $IMAGE_TAG|g" kustomization.yaml
            
            echo "  âœ… Kustomization updated with tag: $IMAGE_TAG"
          fi

      - name: Commit and Push Changes
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          git add -A
          git diff --staged --quiet || git commit -m "Deploy ${{ inputs.app_name }} to ${{ inputs.app_env }} - ${{ github.sha }}"
          git pull --rebase origin ${{ github.ref_name }} || true
          git push || true

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # PHASE 3: VERIFICATION
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  verification:
    name: "Phase 3: Verification"
    runs-on: ubuntu-latest
    needs: [infrastructure, application]

    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ inputs.app_region }}

      - name: Setup kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/v1.28.0/bin/linux/amd64/kubectl"
          chmod +x kubectl && sudo mv kubectl /usr/local/bin/

      - name: Create Namespace and AWS Credentials Secret
        run: |
          aws eks update-kubeconfig --name "${{ needs.infrastructure.outputs.workload_cluster }}" --region ${{ inputs.app_region }}

          NAMESPACE="${{ needs.infrastructure.outputs.namespace }}"

          # Create namespace if not exists
          kubectl create namespace $NAMESPACE --dry-run=client -o yaml | kubectl apply -f -

          # Learning #138-143: Create AWS credentials secret BEFORE ArgoCD sync
          kubectl delete secret aws-credentials -n $NAMESPACE --ignore-not-found
          kubectl create secret generic aws-credentials \
            --namespace $NAMESPACE \
            --from-literal=AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID }} \
            --from-literal=AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY }}

          echo "  âœ… Namespace and secrets created: $NAMESPACE"

      - name: Verify Infrastructure
        run: |
          echo ""
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "  INFRASTRUCTURE VERIFICATION"
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          
          echo ""
          echo "ArgoCD Cluster: ${{ needs.infrastructure.outputs.argocd_cluster }}"
          aws eks describe-cluster --name "${{ needs.infrastructure.outputs.argocd_cluster }}" \
            --query 'cluster.status' --output text || echo "NOT FOUND"

          echo ""
          echo "Workload Cluster: ${{ needs.infrastructure.outputs.workload_cluster }}"
          aws eks describe-cluster --name "${{ needs.infrastructure.outputs.workload_cluster }}" \
            --query 'cluster.status' --output text || echo "NOT FOUND"

          echo ""
          echo "ECR Repository: ${{ needs.infrastructure.outputs.ecr_repo }}"
          aws ecr describe-repositories --repository-names "${{ needs.infrastructure.outputs.ecr_repo }}" \
            --query 'repositories[0].repositoryUri' --output text || echo "NOT FOUND"

      - name: Deployment Summary
        run: |
          echo ""
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "  ðŸŽ® DEPLOYMENT JOURNEY COMPLETE - GKOrgApp1"
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo ""
          echo "  ðŸ“¦ Application: ${{ inputs.app_name }}"
          echo "  ðŸŒ Environment: ${{ inputs.app_env }}"
          echo "  ðŸ¢ Tenant: ${{ inputs.tenant_name }}"
          echo "  ðŸ“ Region: ${{ inputs.app_region }}"
          echo ""
          echo "  ðŸ”— Resources Created:"
          echo "     â€¢ ArgoCD Cluster: ${{ needs.infrastructure.outputs.argocd_cluster }}"
          echo "     â€¢ Workload Cluster: ${{ needs.infrastructure.outputs.workload_cluster }}"
          echo "     â€¢ ECR: ${{ needs.infrastructure.outputs.ecr_repo }}"
          echo "     â€¢ Namespace: ${{ needs.infrastructure.outputs.namespace }}"
          echo ""
          echo "  ðŸŒ Endpoint: https://${{ inputs.app_name }}-${{ inputs.app_env }}.agents.opsera-labs.com"
          echo ""
          echo "  ðŸ“Š Deployment Type: ${{ needs.infrastructure.outputs.deployment_type }}"
          echo ""
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

          # Write to job summary
          cat >> $GITHUB_STEP_SUMMARY << EOF
          ## ðŸŽ® Deployment Complete - GKOrgApp1

          | Resource | Value |
          |----------|-------|
          | Application | ${{ inputs.app_name }} |
          | Environment | ${{ inputs.app_env }} |
          | Tenant | ${{ inputs.tenant_name }} |
          | Region | ${{ inputs.app_region }} |
          | ArgoCD Cluster | ${{ needs.infrastructure.outputs.argocd_cluster }} |
          | Workload Cluster | ${{ needs.infrastructure.outputs.workload_cluster }} |
          | ECR Repository | ${{ needs.infrastructure.outputs.ecr_repo }} |
          | Namespace | ${{ needs.infrastructure.outputs.namespace }} |
          | Deployment Type | ${{ needs.infrastructure.outputs.deployment_type }} |
          | Endpoint | https://${{ inputs.app_name }}-${{ inputs.app_env }}.agents.opsera-labs.com |
          EOF
